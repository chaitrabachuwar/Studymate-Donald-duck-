# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cI5PVyWCSLOM4KKwIHura5vIwZ1zEtbU
"""

# === Single Colab cell: Study Mate (Translation, PDF Q/A, PDF Summary, IMAGE SUMMARY) ===
# Copy & paste this whole cell into Google Colab and run.

# ------------------- 1) INSTALL DEPENDENCIES -------------------
!apt-get update -qq
!apt-get install -y -qq poppler-utils tesseract-ocr

!pip install --quiet "transformers>=4.49" accelerate huggingface_hub gradio pillow PyPDF2 pdf2image pytesseract
!pip install --quiet torch --index-url https://download.pytorch.org/whl/cpu

# Image model
!pip install --quiet timm

# ------------------- 2) IMPORTS -------------------
import os, io
from typing import List
import torch
from transformers import pipeline
from huggingface_hub import login as hf_login
from PyPDF2 import PdfReader
from pdf2image import convert_from_bytes
import pytesseract
from PIL import Image
import gradio as gr


# ------------------- 3) CONFIG -------------------
TEXT_MODEL_ID = "ibm-granite/granite-3.2-2b-instruct"
IMG_MODEL_ID = "Salesforce/blip-image-captioning-large"

HF_TOKEN = os.environ.get("HF_TOKEN", None)
if HF_TOKEN:
    try:
        hf_login(HF_TOKEN)
        print("[INFO] HuggingFace token authenticated.")
    except:
        print("[WARN] Failed to authenticate HF token")

DEVICE = 0 if torch.cuda.is_available() else -1
print("Using device:", "CUDA" if DEVICE == 0 else "CPU")

# ------------------- 4) TEXT MODEL LOADER -------------------
_text_pipe = None
def load_text_pipe():
    global _text_pipe
    if _text_pipe is None:
        print("[INFO] Loading text model...")
        _text_pipe = pipeline(
            "text-generation",
            model=TEXT_MODEL_ID,
            device=DEVICE,
            trust_remote_code=True,
            max_new_tokens=512,
            do_sample=False
        )
        print("[INFO] Text model loaded.")
    return _text_pipe

def run_instruction(prompt, max_new_tokens=512):
    pipe = load_text_pipe()
    out = pipe(prompt, max_new_tokens=max_new_tokens)
    if isinstance(out, list) and "generated_text" in out[0]:
        return out[0]["generated_text"].strip()
    return str(out)

# ------------------- 5) IMAGE MODEL LOADER -------------------
_img_pipe = None
def load_image_pipe():
    global _img_pipe
    if _img_pipe is None:
        print("[INFO] Loading image captioning model...")
        _img_pipe = pipeline(
            "image-to-text",
            model=IMG_MODEL_ID,
            device=DEVICE
        )
        print("[INFO] Image model loaded.")
    return _img_pipe

# ------------------- 6) IMAGE SUMMARY -------------------
def summarize_image(image):
    if image is None:
        return "Upload an image first."

    img_pipe = load_image_pipe()

    # base caption
    caption = img_pipe(image)[0]['generated_text']

    # improve caption using Granite LLM
    prompt = f"""
You are an expert vision assistant. Improve the following image caption into a detailed, clear image summary:

Caption: "{caption}"

Provide a descriptive and advanced image summary.
"""
    final_summary = run_instruction(prompt, max_new_tokens=300)
    return final_summary


# ------------------- 7) PDF EXTRACTOR -------------------
def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    try:
        reader = PdfReader(io.BytesIO(pdf_bytes))
        pages = [p.extract_text() or "" for p in reader.pages]
        combined = "\n\n".join(pages).strip()
    except:
        combined = ""

    # OCR fallback
    if len(combined.strip()) < 50:
        try:
            images = convert_from_bytes(pdf_bytes, dpi=200)
            ocr_texts = [pytesseract.image_to_string(img) for img in images]
            combined = "[OCR extracted text]\n\n" + "\n\n".join(ocr_texts)
        except:
            pass
    return combined


# ---------- helper: chunk PDF ----------
def chunk_text(text, max_chars=3000):
    chunks, start, L = [], 0, len(text)
    while start < L:
        end = min(start + max_chars, L)
        cut = text.rfind("\n", start, end)
        if cut <= start: cut = text.rfind(".", start, end)
        if cut <= start: cut = end
        chunks.append(text[start:cut].strip())
        start = cut
    return chunks

def choose_best_chunk(chunks, question):
    if not chunks: return ""
    q = set(w.lower() for w in question.split())
    best, score = 0, -1
    for i, c in enumerate(chunks):
        s = len(q & set(c.lower().split()))
        if s > score:
            best, score = i, s
    return chunks[best]


# ------------------- 8) TRANSLATION -------------------
def translate_text(user_text, target_language="English"):
    if not user_text.strip():
        return "Provide text to translate."
    prompt = f"""
Translate the following into {target_language}:

\"\"\"{user_text}\"\"\"
Provide only the translated text.
"""
    return run_instruction(prompt)


# ------------------- 9) PDF Q/A -------------------
def pdf_qa(pdf_file, question):
    if pdf_file is None:
        return "Upload a PDF first."

    with open(pdf_file.name, "rb") as f:
        b = f.read()

    if not question.strip():
        return "Enter a question."

    pdf_text = extract_text_from_pdf_bytes(b)
    chunks = chunk_text(pdf_text)
    best = choose_best_chunk(chunks, question)

    prompt = f"""
Answer the question strictly using this excerpt:

\"\"\"{best}\"\"\"

Question: {question}

If answer not found, say: "I cannot find the answer in the supplied material."
"""
    return run_instruction(prompt)


# ------------------- 10) PDF SUMMARY -------------------
def summarize_pdf(pdf_file, length="short"):
    if pdf_file is None:
        return "Upload a PDF first."

    with open(pdf_file.name, "rb") as f:
        b = f.read()

    pdf_text = extract_text_from_pdf_bytes(b)
    style = "detailed" if length == "long" else "short"

    if len(pdf_text) < 6000:
        prompt = f"Provide a {style} summary:\n\n{pdf_text}"
        return run_instruction(prompt)

    chunks = chunk_text(pdf_text)
    summaries = []

    for c in chunks:
        p = f"Summarize this in 2â€“4 sentences:\n\n{c}"
        summaries.append(run_instruction(p, max_new_tokens=200))

    final_prompt = f"Combine these into one {style} summary:\n\n{summaries}"
    return run_instruction(final_prompt)


# ------------------- 11) GRADIO UI -------------------
with gr.Blocks(title="Study Mate â€” AI Study Assistant") as demo:
    gr.Markdown("# ðŸ“˜ Study Mate (Advanced Version)\nTranslate text, ask Q/A from PDFs, summarize PDFs, and *summarize images*.")

    with gr.Tabs():

        # Translate
        with gr.TabItem("Translate"):
            inp = gr.Textbox(lines=5, label="Enter text")
            lang = gr.Textbox(label="Target language", value="English")
            btn = gr.Button("Translate")
            out = gr.Textbox(lines=5, label="Translation")
            btn.click(translate_text, [inp, lang], out)

        # PDF Q/A
        with gr.TabItem("PDF Q/A"):
            pdf = gr.File(label="Upload PDF")
            q = gr.Textbox(label="Ask a question")
            btn2 = gr.Button("Ask")
            ans = gr.Textbox(label="Answer")
            btn2.click(pdf_qa, [pdf, q], ans)

        # PDF Summary
        with gr.TabItem("PDF Summary"):
            pdf2 = gr.File(label="Upload PDF")
            style = gr.Radio(["short", "long"], value="short")
            btn3 = gr.Button("Summarize")
            out2 = gr.Textbox(lines=8, label="Summary")
            btn3.click(summarize_pdf, [pdf2, style], out2)

        # Image Summary
        with gr.TabItem("Image Summary"):
            img = gr.Image(type="pil", label="Upload Image")
            img_btn = gr.Button("Summarize Image")
            img_out = gr.Textbox(lines=8, label="Image Summary")
            img_btn.click(summarize_image, [img], img_out)

demo.launch(share=False, inbrowser=True)